# üì° DOCUMENTACI√ìN COMPLETA DE COMUNICACIONES FRONTEND

## RESUMEN EJECUTIVO

El frontend se comunica con **4 servidores principales**:

1. **backend.movilive.es** - Backend OpenAI (HTTP REST)
2. **voz.movilive.es** - Servidor TTS Audio (WebSocket)
3. **avoz.movilive.es** - Servidor TTS + Avatar (HTTP REST)
4. **avatar.movilive.es** - Servidor Video Avatar (WebRTC + HTTP)

---

## 1Ô∏è‚É£ BACKEND.MOVILIVE.ES (Backend OpenAI)

### Descripci√≥n
Backend principal que procesa todas las consultas con OpenAI GPT-4, maneja autenticaci√≥n, suscripciones y cr√©ditos.

### Endpoints

#### üìç POST /api/welcome
**Prop√≥sito:** Obtener mensaje de bienvenida personalizado

**Archivo:** `src/services/WelcomeService.ts` (l√≠neas 25-34)

```typescript
const response = await fetch(`${this.baseUrl}/api/welcome`, {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    lang: language,        // "es" | "en" | "pt" | "it" | "de" | "fr"
    name: string,          // Nombre del usuario
    gender: string,        // "male" | "female"
    hour: number,          // new Date().getHours()
  }),
});

const data = await response.json();
```

**Respuesta:**
```json
{
  "message": "Buenos d√≠as, Juan. Es un placer saludarte...",
  "response": "...",
  "bible": {
    "text": "Porque de tal manera am√≥ Dios...",
    "ref": "Juan 3:16"
  },
  "question": "¬øEn qu√© puedo ayudarte hoy?",
  "sessionId": "session-1234567890"
}
```

---

#### üìç POST /api/ask
**Prop√≥sito:** Enviar pregunta de usuario y recibir respuesta de GPT-4

**Archivo:** `src/Chat.tsx` (l√≠neas 436-447)

```typescript
const r = await fetch(`${backendUrl}/api/ask`, {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  credentials: "omit",
  body: JSON.stringify({
    message: userText,         // Pregunta del usuario
    lang: lang,                // Idioma actual
    sessionId: sessionId,      // ID de sesi√≥n
    name: userName,            // Nombre del usuario
    gender: userGender,        // G√©nero del usuario
    mode: currentMode,         // "chat" | "audio" | "video" | "video-chat"
  }),
});

const data = await r.json();
```

**Respuesta:**
```json
{
  "message": "Respuesta de Jes√∫s con amor y sabidur√≠a...",
  "question": "Pregunta original del usuario",
  "bible": {
    "text": "Texto b√≠blico relacionado",
    "ref": "Libro Cap√≠tulo:Vers√≠culo"
  }
}
```

**Uso:**
- Modo Chat: Env√≠a texto escrito
- Modo Audio: Env√≠a texto transcrito por Speech Recognition nativo
- **NUEVO Android Whisper:** Usar√° `/api/transcribe` en lugar de `/api/ask`

---

#### üìç POST /api/transcribe (PENDIENTE IMPLEMENTAR)
**Prop√≥sito:** Transcribir audio con Whisper + procesar con GPT en un solo request

**Archivo:** `src/services/TranscriptionService.ts` (l√≠neas 20-40)

```typescript
const formData = new FormData();
const audioFile = new File([audioBlob], `audio.webm`, { type: audioBlob.type });

formData.append('audio', audioFile);
formData.append('lang', language);
formData.append('mode', mode);
formData.append('history', JSON.stringify(history));
formData.append('sessionId', sessionId);
formData.append('name', name);
formData.append('gender', gender);

const response = await fetch(`${BACKEND_URL}/api/transcribe`, {
  method: 'POST',
  body: formData  // ‚ö†Ô∏è NO enviar Content-Type, FormData lo maneja
});

const result = await response.json();
```

**Respuesta esperada:**
```json
{
  "message": "Respuesta de Jes√∫s...",
  "question": "Transcripci√≥n del audio del usuario",
  "bible": {
    "text": "Texto b√≠blico",
    "ref": "Referencia"
  },
  "followUpQuestion": "¬øQu√© m√°s te gustar√≠a compartir?"
}
```

**IMPORTANTE:** El backend debe incluir:
- `question`: Transcripci√≥n del audio (para mostrar como mensaje del usuario)
- `followUpQuestion`: Pregunta de seguimiento (para incluir en respuesta del asistente)

Campos alternativos aceptados para pregunta de seguimiento:
- `followUpQuestion` (recomendado)
- `nextQuestion`
- `follow_up_question`
- `followup_question`

**Flujo interno del backend:**
1. Recibe audio ‚Üí Whisper API ‚Üí transcripci√≥n
2. Transcripci√≥n ‚Üí GPT-4 con historial ‚Üí respuesta
3. Distribuci√≥n seg√∫n mode:
   - `chat`: Solo retorna texto
   - `audio`: Env√≠a a voz.movilive.es
   - `video`: Env√≠a a avoz.movilive.es
   - `video-chat`: Env√≠a a avoz.movilive.es

---

## 2Ô∏è‚É£ VOZ.MOVILIVE.ES (Servidor TTS Audio)

### Descripci√≥n
Servidor WebSocket que recibe texto y streaming de audio PCM16 en tiempo real.

### Conexi√≥n WebSocket

**Archivo:** `src/App.tsx` (l√≠neas 418-503)

#### URL de conexi√≥n:
```
wss://voz.movilive.es/ws/synthesize
```

#### Clase TTSWSClient

```typescript
class TTSWSClient {
  private url = "wss://voz.movilive.es/ws/synthesize";
  private ws: WebSocket | null = null;

  connect() {
    this.ws = new WebSocket(this.url);
    this.ws.binaryType = "arraybuffer";

    this.ws.onopen = () => {
      console.log("[WS] Conexi√≥n establecida");
    };

    this.ws.onmessage = async (ev) => {
      if (ev.data instanceof ArrayBuffer) {
        // Audio PCM16 recibido
        const pcm16 = new Int16Array(ev.data);
        await this.player.enqueuePCM16(pcm16, channels, sampleRate);
      } else {
        // Mensaje de control JSON
        const data = JSON.parse(ev.data);
        if (data.type === "start") {
          // Formato: { type: "start", sample_rate: 24000, channels: 1 }
        } else if (data.type === "end") {
          // Audio completado
        }
      }
    };
  }

  synthesize(payload: { text: string; lang: Language; sessionId: string }) {
    const msg = {
      text: payload.text,
      voice_id: "jesus_default",
      speed: 1.0,
      lang: payload.lang,
      sessionId: payload.sessionId,
    };
    this.ws.send(JSON.stringify(msg));
  }
}
```

#### Env√≠o de mensaje (Chat.tsx l√≠neas 867-897)

```typescript
const payload = {
  text: first,              // Texto a sintetizar
  lang: lang,               // Idioma
  route: "audio_on",        // Modo audio
  sessionId: sessionId,     // ID de sesi√≥n
};
ttsChannel.send(JSON.stringify(payload));
```

**Formato de audio recibido:**
- Tipo: PCM16 (Int16Array)
- Sample Rate: 24000 Hz (configurable)
- Canales: 1 (mono)
- Formato: ArrayBuffer

**Reproducci√≥n:**
El frontend usa `AudioQueuePlayer` (l√≠neas 346-413) para:
1. Recibir chunks PCM16
2. Resamplear a sample rate del navegador
3. Programar reproducci√≥n con Web Audio API
4. Reproducir en streaming sin esperar archivo completo

---

## 3Ô∏è‚É£ AVOZ.MOVILIVE.ES (Servidor TTS + Avatar)

### Descripci√≥n
Servidor HTTP que recibe texto y lo env√≠a DIRECTAMENTE al servidor de avatar a trav√©s del canal WebRTC ya establecido.

### Endpoint

#### üìç POST /synthesize_to_livetalking
**Prop√≥sito:** Sintetizar voz y enviarla al avatar en tiempo real

**Archivo:** `src/Chat.tsx` (l√≠neas 370-456)

```typescript
async function sendToLiveTalking(text: string, lang: Language, sessionId: number) {
  const response = await fetch(`${AVOZ_URL}/synthesize_to_livetalking`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      text: text,            // Texto a sintetizar
      lang: lang,            // Idioma ("es", "en", etc)
      sessionid: sessionId   // ‚ö†Ô∏è IMPORTANTE: ID de sesi√≥n WebRTC
    }),
  });

  if (!response.ok) {
    throw new Error(`Error: ${response.status}`);
  }

  const data = await response.json();
  return data;
}
```

**Request Body:**
```json
{
  "text": "Hola Juan, me alegra que me preguntes...",
  "lang": "es",
  "sessionid": 1234567890
}
```

**Response:**
```json
{
  "status": "success",
  "duration": 3500
}
```

**Flujo interno:**
1. Backend recibe texto
2. Sintetiza con XTTS
3. Env√≠a audio directamente al canal WebRTC del avatar (usando sessionid)
4. Avatar recibe audio y lo reproduce con sincronizaci√≥n labial

**‚ö†Ô∏è CR√çTICO:**
- El `sessionid` debe ser el mismo que se obtuvo al negociar WebRTC con avatar.movilive.es
- Si el sessionid es inv√°lido o la conexi√≥n WebRTC se cerr√≥, retorna error 500

---

## 4Ô∏è‚É£ AVATAR.MOVILIVE.ES (Servidor Video Avatar)

### Descripci√≥n
Servidor WebRTC que transmite video del avatar con sincronizaci√≥n labial.

### Endpoints

#### üìç POST /offer
**Prop√≥sito:** Negociar conexi√≥n WebRTC (SDP Offer/Answer)

**Archivo:** `src/App.tsx` (l√≠neas 1067-1108)

```typescript
async function negotiateAvatarPC(pc: RTCPeerConnection) {
  // 1. Crear oferta local
  const offer = await pc.createOffer();
  await pc.setLocalDescription(offer);

  // 2. Enviar oferta al servidor
  const r = await fetch("https://avatar.movilive.es/offer", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      sdp: offer.sdp,
      type: offer.type  // "offer"
    }),
  });

  // 3. Recibir respuesta del servidor
  const data = await r.json();
  // { type: "answer", sdp: "...", sessionid: 1234567890 }

  // 4. Establecer respuesta remota
  await pc.setRemoteDescription(
    new RTCSessionDescription({
      type: data.type,
      sdp: data.sdp
    })
  );

  return data.sessionid;
}
```

**Request:**
```json
{
  "sdp": "v=0\r\no=- 123456789 2 IN IP4...",
  "type": "offer"
}
```

**Response:**
```json
{
  "type": "answer",
  "sdp": "v=0\r\no=- 987654321 2 IN IP4...",
  "sessionid": 1234567890
}
```

---

#### üìç /videostream (Alternativo)
**Prop√≥sito:** Endpoint alternativo si /offer falla

**Archivo:** `src/App.tsx` (l√≠neas 23-26)

```typescript
const AVATAR_OFFER_URLS = [
  "https://avatar.movilive.es/offer",
  "https://avatar.movilive.es/videostream",  // Fallback
];
```

**Uso:** El sistema intenta `/offer` primero. Si falla, intenta `/videostream` con el mismo formato.

---

### Configuraci√≥n WebRTC

**Archivo:** `src/App.tsx` (l√≠neas 1110-1200)

```typescript
const startAvatarWebRTC = async (): Promise<number | null> => {
  const pc = new RTCPeerConnection({
    iceServers: [
      { urls: "stun:stun.l.google.com:19302" },
      { urls: "stun:stun1.l.google.com:19302" },
    ],
    bundlePolicy: "max-bundle",
  });

  // Recibir audio y video
  pc.addTransceiver("audio", { direction: "recvonly" });
  pc.addTransceiver("video", { direction: "recvonly" });

  // Manejar tracks recibidos
  pc.ontrack = (event) => {
    console.log("[Avatar] Track recibido:", event.track.kind);
    if (event.streams && event.streams[0]) {
      avatarStreamRef.current = event.streams[0];
      setHasAvatarStream(true);

      // Asignar al elemento <video>
      const vid = document.getElementById("avatar-video") as HTMLVideoElement;
      vid.srcObject = event.streams[0];
      vid.play();
    }
  };

  // Negociar
  const sessionId = await negotiateAvatarPC(pc);
  return sessionId;
};
```

**MediaStream recibido:**
- Video: 720x1280 (portrait)
- Audio: Sincronizado con movimiento labial
- Codec: Depende del navegador (VP8/VP9/H.264)

---

### Renderizado del Video

**Archivo:** `src/components/AvatarVideo.tsx` (l√≠neas 262-276)

```typescript
<video
  ref={avatarVideoRef}
  autoPlay
  playsInline
  style={{
    width: "100%",
    height: "100%",
    objectFit: "cover",
    transform: `scale(${zoom}) translateY(${offsetY}px)`,
  }}
/>
```

**Video de reposo (cuando no hay audio):**
```typescript
<video
  src="https://backend.movilive.es/static/reposo_final_720x1280.mp4"
  autoPlay
  loop
  muted
  playsInline
/>
```

---

## üìä DIAGRAMA DE FLUJOS

### Flujo 1: Modo Chat (Solo texto)
```
Usuario escribe ‚Üí /api/ask ‚Üí GPT-4 ‚Üí Respuesta texto
```

### Flujo 2: Modo Audio (Voz sin video)
```
Usuario habla ‚Üí Speech Recognition ‚Üí /api/ask ‚Üí GPT-4
  ‚Üí Respuesta texto ‚Üí voz.movilive.es (WebSocket)
  ‚Üí Audio PCM16 streaming ‚Üí Reproducci√≥n
```

### Flujo 3: Modo Video (Video sin chat)
```
[Inicio]
1. Frontend ‚Üí avatar.movilive.es/offer ‚Üí Negociaci√≥n WebRTC
2. Backend ‚Üí sessionid=1234567890
3. Frontend recibe MediaStream (video + audio)

[Al hacer pregunta]
Usuario habla ‚Üí Speech Recognition ‚Üí /api/ask ‚Üí GPT-4
  ‚Üí Respuesta texto ‚Üí avoz.movilive.es/synthesize_to_livetalking
  ‚Üí Backend sintetiza + env√≠a a WebRTC ‚Üí Avatar habla con sincronizaci√≥n labial
```

### Flujo 4: Modo Video-Chat (Video + Chat habilitado)
```
Igual que Modo Video, pero con chat visible para escribir
```

### Flujo 5: Android con Whisper (NUEVO - PENDIENTE)
```
Usuario habla ‚Üí MediaRecorder (webm) ‚Üí /api/transcribe
  ‚Üí Backend: Whisper + GPT-4 + Distribuci√≥n
  ‚Üí Respuesta seg√∫n mode (texto/audio/video)
```

---

## üîê AUTENTICACI√ìN Y CR√âDITOS

Todos los endpoints de backend.movilive.es usan `deviceId` para identificar al usuario:

**Archivo:** `src/services/SubscriptionService.ts`

```typescript
// Obtener suscripci√≥n actual
GET /api/subscription/{deviceId}

// Crear usuario nuevo
POST /api/users
Body: { device_id, name, gender, language }

// Consumir cr√©ditos
POST /api/credits/consume
Body: { device_id, credits, mode }
```

---

## üéØ RESUMEN DE PUERTOS Y PROTOCOLOS

| Servidor | Puerto | Protocolo | Prop√≥sito |
|----------|--------|-----------|-----------|
| backend.movilive.es | 443 (HTTPS) | HTTP REST | OpenAI GPT-4, autenticaci√≥n, cr√©ditos |
| voz.movilive.es | 443 (WSS) | WebSocket | Streaming audio TTS |
| avoz.movilive.es | 443 (HTTPS) | HTTP REST | TTS + env√≠o a avatar |
| avatar.movilive.es | 443 (HTTPS) | WebRTC + HTTP | Video avatar en tiempo real |

---

## üöÄ PR√ìXIMOS PASOS: Integraci√≥n Whisper

### Backend necesita implementar:

```python
@app.post("/api/transcribe")
async def transcribe_audio(
    audio: UploadFile,
    lang: str = Form(...),
    mode: str = Form(...),
    history: str = Form(...),
    sessionId: str = Form(...),
    name: str = Form(...),
    gender: str = Form(...)
):
    # 1. Recibir audio
    audio_bytes = await audio.read()

    # 2. Transcribir con Whisper
    transcript = openai.Audio.transcribe(
        model="whisper-1",
        file=audio_bytes,
        language=lang
    )

    # 3. Procesar con GPT-4 (igual que /api/ask)
    response = await process_with_gpt(
        message=transcript.text,
        history=json.loads(history),
        lang=lang,
        name=name,
        gender=gender
    )

    # 4. Distribuir seg√∫n mode
    if mode == "video" or mode == "video-chat":
        # Enviar a avoz.movilive.es
        await synthesize_to_livetalking(response["message"], lang, sessionId)
    elif mode == "audio":
        # Enviar a voz.movilive.es (o retornar audio)
        pass

    # 5. Retornar respuesta
    return {
        "message": response["message"],
        "question": transcript.text,
        "bible": response["bible"]
    }
```

---

## üìù NOTAS IMPORTANTES

1. **SessionId cr√≠tico:** El sessionId de WebRTC debe mantenerse en sincron√≠a entre:
   - Frontend (livetalkSessionId)
   - Avatar.movilive.es (conexi√≥n WebRTC)
   - Avoz.movilive.es (para enviar audio)

2. **Reconexi√≥n autom√°tica:**
   - voz.movilive.es (WebSocket): Se reconecta autom√°ticamente cada 1.5s
   - avatar.movilive.es (WebRTC): Se reconecta manualmente con retry

3. **Detecci√≥n de inactividad:**
   - 5 minutos sin interacci√≥n ‚Üí Cierra WebRTC para ahorrar cr√©ditos
   - Modal blanco avisa al usuario

4. **Cr√©ditos:**
   - Se consumen DESPU√âS de que el audio termina de reproducirse
   - Chat: 1 cr√©dito, Audio: 2 cr√©ditos, Video: 4 cr√©ditos, Video-Chat: 4 cr√©ditos

---

**Fecha:** 2025-01-23
**Versi√≥n:** 1.0
**Actualizado por:** Sistema de documentaci√≥n autom√°tica
